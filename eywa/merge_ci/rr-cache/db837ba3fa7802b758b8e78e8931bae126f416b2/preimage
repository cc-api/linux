// SPDX-License-Identifier: MIT
/*
 * Copyright Â© 2020 Intel Corporation
 */

#include "gem/i915_gem_ioctls.h"
#include "gem/i915_gem_region.h"

#include "i915_drv.h"

static int
i915_gem_create(struct drm_file *file,
		struct intel_memory_region *mr,
		u64 *size_p,
		u32 *handle_p)
{
	struct drm_i915_gem_object *obj;
	u32 handle;
	u64 size;
	int ret;

	GEM_BUG_ON(!is_power_of_2(mr->min_page_size));
	size = round_up(*size_p, mr->min_page_size);
	if (size == 0)
		return -EINVAL;

	/* For most of the ABI (e.g. mmap) we think in system pages */
	GEM_BUG_ON(!IS_ALIGNED(size, PAGE_SIZE));

	/* Allocate the new object */
	obj = i915_gem_object_create_region(mr, size, 0);
	if (IS_ERR(obj))
		return PTR_ERR(obj);

<<<<<<<
	GEM_BUG_ON(size != obj->base.size);
=======
	if (i915_gem_object_is_lmem(obj)) {
		struct intel_gt *gt = obj->mm.region->gt;
		struct intel_context *ce = gt->engine[BCS0]->blitter_context;

		/*
		 * XXX: We really want to move this to get_pages(), but we
		 * require grabbing the BKL for the blitting operation which is
		 * annoying. In the pipeline is support for async get_pages()
		 * which should fit nicely for this. Also note that the actual
		 * clear should be done async(we currently do an object_wait
		 * which is pure garbage), we just need to take care if
		 * userspace opts of implicit sync for the execbuf, to avoid any
		 * potential info leak.
		 */

retry:
		ret = i915_gem_object_fill_blt(obj, ce, 0);
		if (ret == -EINTR)
			goto retry;
		if (ret) {
			/*
			 * XXX: Post the error to where we would normally gather
			 * and clear the pages. This better reflects the final
			 * uapi behaviour, once we are at the point where we can
			 * move the clear worker to get_pages().
			 */
			i915_gem_object_unbind(obj, I915_GEM_OBJECT_UNBIND_ACTIVE);
			i915_gem_object_lock(obj, NULL);
			__i915_gem_object_put_pages(obj);
			i915_gem_object_unlock(obj);
			obj->mm.gem_create_posted_err = ret;
			goto handle_create;
		}

		/*
		 * XXX: Occasionally i915_gem_object_wait() called inside
		 * i915_gem_object_set_to_cpu_domain() get interrupted
		 * and return -ERESTARTSYS, this will cause go clearing
		 * code below and also set the gem_create_posted_err.
		 * moreover, the clearing sometimes fails because the
		 * object is still pinned by the blitter clearing code.
		 * this makes us to have an object with or without lmem
		 * pages, and with gem_create_posted_err = -ERESTARTSYS.
		 * Under lmem pressure, if the object has pages, we might
		 * swap out this object to smem. Next when user space
		 * code use this object in gem_execbuf() call, get_pages()
		 * operation will return -ERESTARTSYS error code, which
		 * causes user space code to fail.
		 *
		 * To avoid this problem, we add a non-interruptible
		 * wait before setting object to cpu domain.
		 */
		i915_gem_object_lock(obj, NULL);
		ret = i915_gem_object_wait(obj, 0, MAX_SCHEDULE_TIMEOUT);
		if (!ret)
			ret = i915_gem_object_set_to_cpu_domain(obj, NULL, false);
		if (ret) {
			i915_gem_object_unbind(obj, I915_GEM_OBJECT_UNBIND_ACTIVE);
			__i915_gem_object_put_pages(obj);
			obj->mm.gem_create_posted_err = ret;
			i915_gem_object_unlock(obj);
			goto handle_create;
		}
		i915_gem_object_unlock(obj);
	}

handle_create:
	obj->mm.placements = placements;
	obj->mm.n_placements = n_placements;
>>>>>>>

	ret = drm_gem_handle_create(file, &obj->base, &handle);
	/* drop reference from allocate - handle holds it now */
	i915_gem_object_put(obj);
	if (ret)
		return ret;

	*handle_p = handle;
	*size_p = size;
	return 0;
}

int
i915_gem_dumb_create(struct drm_file *file,
		     struct drm_device *dev,
		     struct drm_mode_create_dumb *args)
{
	enum intel_memory_type mem_type;
	int cpp = DIV_ROUND_UP(args->bpp, 8);
	u32 format;

	switch (cpp) {
	case 1:
		format = DRM_FORMAT_C8;
		break;
	case 2:
		format = DRM_FORMAT_RGB565;
		break;
	case 4:
		format = DRM_FORMAT_XRGB8888;
		break;
	default:
		return -EINVAL;
	}

	/* have to work out size/pitch and return them */
	args->pitch = ALIGN(args->width * cpp, 64);

	/* align stride to page size so that we can remap */
	if (args->pitch > intel_plane_fb_max_stride(to_i915(dev), format,
						    DRM_FORMAT_MOD_LINEAR))
		args->pitch = ALIGN(args->pitch, 4096);

	if (args->pitch < args->width)
		return -EINVAL;

	args->size = mul_u32_u32(args->pitch, args->height);

	mem_type = INTEL_MEMORY_SYSTEM;
	if (HAS_LMEM(to_i915(dev)))
		mem_type = INTEL_MEMORY_LOCAL;

	return i915_gem_create(file,
			       intel_memory_region_by_type(to_i915(dev),
							   mem_type),
			       &args->size, &args->handle);
}

/**
 * Creates a new mm object and returns a handle to it.
 * @dev: drm device pointer
 * @data: ioctl data blob
 * @file: drm file pointer
 */
int
i915_gem_create_ioctl(struct drm_device *dev, void *data,
		      struct drm_file *file)
{
	struct drm_i915_private *i915 = to_i915(dev);
	struct drm_i915_gem_create *args = data;

	i915_gem_flush_free_objects(i915);

	return i915_gem_create(file,
			       intel_memory_region_by_type(i915,
							   INTEL_MEMORY_SYSTEM),
			       &args->size, &args->handle);
}
